# å¼€å§‹è®­ç»ƒ - è¯¦ç»†æ­¥éª¤

## æ–¹æ³• 1: ä½¿ç”¨è®­ç»ƒè„šæœ¬ï¼ˆæœ€ç®€å•ï¼‰â­

### åŸºæœ¬ç”¨æ³•

```bash
./train_libero10.sh [GPU_ID] [SEED] [POLICY] [ALGO]
```

### ç¤ºä¾‹

```bash
# ä½¿ç”¨ GPU 0ï¼Œç§å­ 42ï¼ŒRNN ç­–ç•¥ï¼ŒåŸºç¡€ç®—æ³•
./train_libero10.sh 0 42 bc_rnn_policy base

# ä½¿ç”¨ GPU 3ï¼Œç§å­ 100ï¼ŒTransformer ç­–ç•¥ï¼Œç»éªŒå›æ”¾ç®—æ³•
./train_libero10.sh 3 100 bc_transformer_policy er

# ä½¿ç”¨ GPU 7ï¼Œç§å­ 42ï¼ŒRNN ç­–ç•¥ï¼Œå¤šä»»åŠ¡å­¦ä¹ 
./train_libero10.sh 7 42 bc_rnn_policy multitask
```

---

## æ–¹æ³• 2: ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œï¼ˆå®Œå…¨æ§åˆ¶ï¼‰

### æ­¥éª¤ 1: æ¿€æ´»ç¯å¢ƒ

```bash
conda activate libero
```

### æ­¥éª¤ 2: è®¾ç½® GPU

```bash
# æŒ‡å®šä½¿ç”¨å“ªä¸ª GPUï¼ˆ0-7ï¼‰
export CUDA_VISIBLE_DEVICES=0
export MUJOCO_EGL_DEVICE_ID=0
```

### æ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ

```bash
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base
```

### å®Œæ•´ç¤ºä¾‹

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate libero

# ä½¿ç”¨ GPU 3
export CUDA_VISIBLE_DEVICES=3
export MUJOCO_EGL_DEVICE_ID=3

# å¼€å§‹è®­ç»ƒ
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base
```

---

## å¤š GPU æœåŠ¡å™¨ä½¿ç”¨æŒ‡å— ğŸ–¥ï¸

### æŸ¥çœ‹å¯ç”¨ GPU

```bash
# æŸ¥çœ‹æ‰€æœ‰ GPU
nvidia-smi

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

### æŒ‡å®šå•ä¸ª GPU

```bash
# æ–¹æ³• 1: ä½¿ç”¨ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0  # ä½¿ç”¨ GPU 0
export CUDA_VISIBLE_DEVICES=3  # ä½¿ç”¨ GPU 3
export CUDA_VISIBLE_DEVICES=7  # ä½¿ç”¨ GPU 7

# æ–¹æ³• 2: åœ¨å‘½ä»¤å‰è®¾ç½®
CUDA_VISIBLE_DEVICES=3 python libero/lifelong/main.py ...

# æ–¹æ³• 3: ä½¿ç”¨è®­ç»ƒè„šæœ¬
./train_libero10.sh 3 42 bc_rnn_policy base  # ä½¿ç”¨ GPU 3
```

### åœ¨ 8 å¼ æ˜¾å¡ä¸Šå¹¶è¡Œè®­ç»ƒä¸åŒä»»åŠ¡

#### æ–¹æ¡ˆ 1: è®­ç»ƒä¸åŒçš„ç§å­ï¼ˆæ¨èï¼‰

```bash
# åœ¨ä¸åŒ GPU ä¸ŠåŒæ—¶è®­ç»ƒä¸åŒç§å­
./train_libero10.sh 0 42 bc_rnn_policy base &
./train_libero10.sh 1 100 bc_rnn_policy base &
./train_libero10.sh 2 200 bc_rnn_policy base &
./train_libero10.sh 3 300 bc_rnn_policy base &
./train_libero10.sh 4 400 bc_rnn_policy base &
./train_libero10.sh 5 500 bc_rnn_policy base &
./train_libero10.sh 6 600 bc_rnn_policy base &
./train_libero10.sh 7 700 bc_rnn_policy base &

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
wait
echo "æ‰€æœ‰è®­ç»ƒå®Œæˆï¼"
```

#### æ–¹æ¡ˆ 2: è®­ç»ƒä¸åŒçš„ç®—æ³•

```bash
# åœ¨ä¸åŒ GPU ä¸ŠåŒæ—¶è®­ç»ƒä¸åŒç®—æ³•
./train_libero10.sh 0 42 bc_rnn_policy base &
./train_libero10.sh 1 42 bc_rnn_policy er &
./train_libero10.sh 2 42 bc_rnn_policy ewc &
./train_libero10.sh 3 42 bc_rnn_policy packnet &
./train_libero10.sh 4 42 bc_rnn_policy multitask &
./train_libero10.sh 5 42 bc_transformer_policy base &
./train_libero10.sh 6 42 bc_transformer_policy er &
./train_libero10.sh 7 42 bc_vilt_policy base &

wait
echo "æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆï¼"
```

#### æ–¹æ¡ˆ 3: ä½¿ç”¨æ‰¹é‡è®­ç»ƒè„šæœ¬

åˆ›å»º `train_parallel.sh`ï¼š

```bash
#!/bin/bash

# åœ¨ 8 å¼  GPU ä¸Šå¹¶è¡Œè®­ç»ƒ

echo "å¼€å§‹å¹¶è¡Œè®­ç»ƒ..."

# GPU 0: base + seed 42
./train_libero10.sh 0 42 bc_rnn_policy base > logs/gpu0.log 2>&1 &

# GPU 1: base + seed 100
./train_libero10.sh 1 100 bc_rnn_policy base > logs/gpu1.log 2>&1 &

# GPU 2: er + seed 42
./train_libero10.sh 2 42 bc_rnn_policy er > logs/gpu2.log 2>&1 &

# GPU 3: ewc + seed 42
./train_libero10.sh 3 42 bc_rnn_policy ewc > logs/gpu3.log 2>&1 &

# GPU 4: packnet + seed 42
./train_libero10.sh 4 42 bc_rnn_policy packnet > logs/gpu4.log 2>&1 &

# GPU 5: multitask + seed 42
./train_libero10.sh 5 42 bc_rnn_policy multitask > logs/gpu5.log 2>&1 &

# GPU 6: transformer + seed 42
./train_libero10.sh 6 42 bc_transformer_policy base > logs/gpu6.log 2>&1 &

# GPU 7: vilt + seed 42
./train_libero10.sh 7 42 bc_vilt_policy base > logs/gpu7.log 2>&1 &

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
wait

echo "æ‰€æœ‰è®­ç»ƒå®Œæˆï¼"
echo "æŸ¥çœ‹æ—¥å¿—: ls logs/"
```

ä½¿ç”¨æ–¹æ³•ï¼š

```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# èµ‹äºˆæ‰§è¡Œæƒé™
chmod +x train_parallel.sh

# å¼€å§‹å¹¶è¡Œè®­ç»ƒ
./train_parallel.sh

# ç›‘æ§æ‰€æœ‰ GPU
watch -n 1 nvidia-smi

# æŸ¥çœ‹æŸä¸ª GPU çš„è®­ç»ƒæ—¥å¿—
tail -f logs/gpu0.log
```

---

## ç›‘æ§è®­ç»ƒè¿›åº¦

### æ–¹æ³• 1: å®æ—¶æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f training.log

# æŸ¥çœ‹ç‰¹å®š GPU çš„æ—¥å¿—
tail -f logs/gpu0.log
```

### æ–¹æ³• 2: ç›‘æ§ GPU ä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§æ‰€æœ‰ GPU
watch -n 1 nvidia-smi

# åªæ˜¾ç¤º GPU ä½¿ç”¨ç‡
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv

# æŒç»­ç›‘æ§
watch -n 1 'nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv'
```

### æ–¹æ³• 3: æ£€æŸ¥è®­ç»ƒè¿›åº¦

```bash
# æŸ¥çœ‹æœ€æ–°çš„å®éªŒç›®å½•
ls -lt experiments/LIBERO_10/Sequential/

# æŸ¥çœ‹è®­ç»ƒç»“æœ
ls experiments/LIBERO_10/Sequential/BCRNNPolicy_seed42/run_001/

# æŸ¥çœ‹å·²ä¿å­˜çš„æ¨¡å‹
ls experiments/LIBERO_10/Sequential/BCRNNPolicy_seed42/run_001/*.pth
```

---

## å¸¸è§åœºæ™¯

### åœºæ™¯ 1: å¿«é€Ÿæµ‹è¯•ï¼ˆä½¿ç”¨ 1 ä¸ª GPUï¼‰

```bash
conda activate libero

export CUDA_VISIBLE_DEVICES=0
export MUJOCO_EGL_DEVICE_ID=0

python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    train.n_epochs=5 \
    eval.n_eval=5
```

### åœºæ™¯ 2: æ­£å¼è®­ç»ƒï¼ˆä½¿ç”¨ 1 ä¸ª GPUï¼‰

```bash
./train_libero10.sh 0 42 bc_rnn_policy base
```

### åœºæ™¯ 3: å¹¶è¡Œè®­ç»ƒå¤šä¸ªç§å­ï¼ˆä½¿ç”¨ 3 ä¸ª GPUï¼‰

```bash
./train_libero10.sh 0 42 bc_rnn_policy base &
./train_libero10.sh 1 100 bc_rnn_policy base &
./train_libero10.sh 2 200 bc_rnn_policy base &
wait
```

### åœºæ™¯ 4: å……åˆ†åˆ©ç”¨ 8 å¼  GPU

```bash
# åˆ›å»ºå¹¶è¿è¡Œå¹¶è¡Œè®­ç»ƒè„šæœ¬
./train_parallel.sh
```

---

## GPU é€‰æ‹©å»ºè®®

### æ ¹æ®æ¨¡å‹å¤§å°é€‰æ‹©

| ç­–ç•¥ | æ˜¾å­˜éœ€æ±‚ | æ¨è GPU |
|------|----------|----------|
| bc_rnn_policy | 4-6 GB | ä»»æ„ GPU |
| bc_transformer_policy | 8-12 GB | æ˜¾å­˜ â‰¥ 12GB |
| bc_vilt_policy | 10-16 GB | æ˜¾å­˜ â‰¥ 16GB |

### æŸ¥çœ‹ GPU æ˜¾å­˜

```bash
nvidia-smi --query-gpu=index,name,memory.total --format=csv
```

### é€‰æ‹©ç©ºé—²çš„ GPU

```bash
# æŸ¥çœ‹å“ªäº› GPU ç©ºé—²
nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv

# è‡ªåŠ¨é€‰æ‹©æœ€ç©ºé—²çš„ GPUï¼ˆç¤ºä¾‹è„šæœ¬ï¼‰
FREE_GPU=$(nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits | nl -v 0 | sort -nrk 2 | head -n 1 | cut -f 1)
echo "ä½¿ç”¨ GPU: $FREE_GPU"
./train_libero10.sh $FREE_GPU 42 bc_rnn_policy base
```

---

## è®­ç»ƒå‚æ•°è°ƒæ•´

### åŸºæœ¬å‚æ•°

```bash
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    train.n_epochs=50 \          # è®­ç»ƒè½®æ•°
    train.batch_size=32 \        # æ‰¹æ¬¡å¤§å°
    train.lr=1e-4 \              # å­¦ä¹ ç‡
    eval.n_eval=20 \             # è¯„ä¼°æ¬¡æ•°
    eval.eval_every=5            # æ¯ N è½®è¯„ä¼°ä¸€æ¬¡
```

### å¿«é€Ÿæµ‹è¯•å‚æ•°

```bash
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    train.n_epochs=5 \
    eval.n_eval=5 \
    eval.eval_every=1
```

### é«˜è´¨é‡è®­ç»ƒå‚æ•°

```bash
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    train.n_epochs=100 \
    train.batch_size=64 \
    eval.n_eval=50 \
    eval.eval_every=5
```

---

## åå°è¿è¡Œè®­ç»ƒ

### æ–¹æ³• 1: ä½¿ç”¨ nohup

```bash
nohup ./train_libero10.sh 0 42 bc_rnn_policy base > training_gpu0.log 2>&1 &

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python

# æŸ¥çœ‹æ—¥å¿—
tail -f training_gpu0.log
```

### æ–¹æ³• 2: ä½¿ç”¨ screen

```bash
# åˆ›å»ºæ–°ä¼šè¯
screen -S training_gpu0

# åœ¨ä¼šè¯ä¸­è¿è¡Œè®­ç»ƒ
./train_libero10.sh 0 42 bc_rnn_policy base

# åˆ†ç¦»ä¼šè¯: Ctrl+A, ç„¶åæŒ‰ D

# é‡æ–°è¿æ¥
screen -r training_gpu0

# åˆ—å‡ºæ‰€æœ‰ä¼šè¯
screen -ls
```

### æ–¹æ³• 3: ä½¿ç”¨ tmux

```bash
# åˆ›å»ºæ–°ä¼šè¯
tmux new -s training_gpu0

# åœ¨ä¼šè¯ä¸­è¿è¡Œè®­ç»ƒ
./train_libero10.sh 0 42 bc_rnn_policy base

# åˆ†ç¦»ä¼šè¯: Ctrl+B, ç„¶åæŒ‰ D

# é‡æ–°è¿æ¥
tmux attach -t training_gpu0

# åˆ—å‡ºæ‰€æœ‰ä¼šè¯
tmux ls
```

---

## å®Œæ•´ç¤ºä¾‹ï¼šåœ¨ 8 å¼  GPU ä¸Šè®­ç»ƒ

```bash
#!/bin/bash

# 1. æ¿€æ´»ç¯å¢ƒ
conda activate libero

# 2. åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# 3. åœ¨æ¯ä¸ª GPU ä¸Šå¯åŠ¨è®­ç»ƒ
echo "åœ¨ GPU 0 ä¸Šè®­ç»ƒ base ç®—æ³•..."
CUDA_VISIBLE_DEVICES=0 MUJOCO_EGL_DEVICE_ID=0 \
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    > logs/gpu0_base.log 2>&1 &

echo "åœ¨ GPU 1 ä¸Šè®­ç»ƒ er ç®—æ³•..."
CUDA_VISIBLE_DEVICES=1 MUJOCO_EGL_DEVICE_ID=1 \
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=er \
    > logs/gpu1_er.log 2>&1 &

echo "åœ¨ GPU 2 ä¸Šè®­ç»ƒ ewc ç®—æ³•..."
CUDA_VISIBLE_DEVICES=2 MUJOCO_EGL_DEVICE_ID=2 \
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=ewc \
    > logs/gpu2_ewc.log 2>&1 &

echo "åœ¨ GPU 3 ä¸Šè®­ç»ƒ packnet ç®—æ³•..."
CUDA_VISIBLE_DEVICES=3 MUJOCO_EGL_DEVICE_ID=3 \
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=packnet \
    > logs/gpu3_packnet.log 2>&1 &

echo "åœ¨ GPU 4 ä¸Šè®­ç»ƒ multitask ç®—æ³•..."
CUDA_VISIBLE_DEVICES=4 MUJOCO_EGL_DEVICE_ID=4 \
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=multitask \
    > logs/gpu4_multitask.log 2>&1 &

echo "åœ¨ GPU 5 ä¸Šè®­ç»ƒ transformer ç­–ç•¥..."
CUDA_VISIBLE_DEVICES=5 MUJOCO_EGL_DEVICE_ID=5 \
python libero/lifelong/main.py \
    seed=42 \
    benchmark_name=LIBERO_10 \
    policy=bc_transformer_policy \
    lifelong=base \
    > logs/gpu5_transformer.log 2>&1 &

echo "åœ¨ GPU 6 ä¸Šè®­ç»ƒä¸åŒç§å­..."
CUDA_VISIBLE_DEVICES=6 MUJOCO_EGL_DEVICE_ID=6 \
python libero/lifelong/main.py \
    seed=100 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    > logs/gpu6_seed100.log 2>&1 &

echo "åœ¨ GPU 7 ä¸Šè®­ç»ƒä¸åŒç§å­..."
CUDA_VISIBLE_DEVICES=7 MUJOCO_EGL_DEVICE_ID=7 \
python libero/lifelong/main.py \
    seed=200 \
    benchmark_name=LIBERO_10 \
    policy=bc_rnn_policy \
    lifelong=base \
    > logs/gpu7_seed200.log 2>&1 &

# 4. ç­‰å¾…æ‰€æœ‰è®­ç»ƒå®Œæˆ
echo "æ‰€æœ‰è®­ç»ƒå·²å¯åŠ¨ï¼"
echo "ç›‘æ§ GPU: watch -n 1 nvidia-smi"
echo "æŸ¥çœ‹æ—¥å¿—: tail -f logs/gpu0_base.log"

wait
echo "æ‰€æœ‰è®­ç»ƒå®Œæˆï¼"
```

ä¿å­˜ä¸º `train_all_gpus.sh`ï¼Œç„¶åè¿è¡Œï¼š

```bash
chmod +x train_all_gpus.sh
./train_all_gpus.sh
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: GPU è¢«å ç”¨

```bash
# æŸ¥çœ‹å“ªäº› GPU ç©ºé—²
nvidia-smi

# ä½¿ç”¨ç©ºé—²çš„ GPU
./train_libero10.sh 3 42 bc_rnn_policy base  # ä½¿ç”¨ GPU 3
```

### é—®é¢˜ 2: æ˜¾å­˜ä¸è¶³

```bash
# å‡å°æ‰¹æ¬¡å¤§å°
python libero/lifelong/main.py ... train.batch_size=16

# æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python libero/lifelong/main.py ... policy=bc_rnn_policy
```

### é—®é¢˜ 3: ç¯å¢ƒå˜é‡æœªè®¾ç½®

```bash
# ç¡®ä¿è®¾ç½®äº†è¿™ä¸¤ä¸ªå˜é‡
export CUDA_VISIBLE_DEVICES=0
export MUJOCO_EGL_DEVICE_ID=0

# æˆ–åœ¨å‘½ä»¤å‰è®¾ç½®
CUDA_VISIBLE_DEVICES=0 MUJOCO_EGL_DEVICE_ID=0 python libero/lifelong/main.py ...
```

---

## æ€»ç»“

### æœ€ç®€å•çš„å¼€å§‹æ–¹å¼

```bash
# 1. æŸ¥çœ‹å¯ç”¨ GPU
nvidia-smi

# 2. é€‰æ‹©ä¸€ä¸ªç©ºé—²çš„ GPUï¼ˆä¾‹å¦‚ GPU 3ï¼‰
./train_libero10.sh 3 42 bc_rnn_policy base

# 3. ç›‘æ§è®­ç»ƒ
tail -f training.log
```

### å……åˆ†åˆ©ç”¨ 8 å¼  GPU

```bash
# åˆ›å»ºå¹¶è¿è¡Œå¹¶è¡Œè®­ç»ƒè„šæœ¬
./train_all_gpus.sh

# ç›‘æ§æ‰€æœ‰ GPU
watch -n 1 nvidia-smi
```

ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
